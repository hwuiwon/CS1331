<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Homework 2 - Source Model</title>
  <meta name="viewport" content="width=device-width">

  <!-- syntax highlighting CSS -->
  <link rel="stylesheet" href="../../../css/syntax.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../../css/main.css">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="../../../css/bootstrap.min.css">

  <!-- Custom styles for this template -->
  <link href="css/navbar-fixed-top.css" rel="stylesheet">
  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
      <![endif]-->

       <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

    </head>
    <body>

      <div class="container">
      <p>Source Model</p>

<h2 id="introduction">Introduction</h2>

<p>In this assignment you’ll practice</p>

<ul>
  <li>writing classes,</li>
  <li>using 2-D arrays,</li>
  <li>simple text processing, and</li>
  <li>basic numerical computing issues.</li>
</ul>

<h2 id="problem-description">Problem Description</h2>

<p>You’re interested in natural language processing …</p>

<h2 id="solution-description">Solution Description</h2>

<p>Write a class named <code class="highlighter-rouge">SourceModel</code> reads a file containing a training corpus and builds a first-order Markov chain of the transition probabilities between letters in the corpus. Only alphabetic characters in the corpus should be considered and they should be normalized to upper or lower case. For simplicity (see background) only consider the 26 letters of the English alphabet.</p>

<h3 id="downloads">Downloads</h3>

<p>Here are some example corpus files and test files:</p>

<ul>
  <li>English: <a href="english.corpus">english.corpus</a></li>
  <li>French: <a href="french.corpus">french.corpus</a></li>
  <li>Spanish: <a href="spanish.corpus">spanish.corpus</a></li>
  <li>HipHop: <a href="hiphop.corpus">hiphop.corpus</a></li>
  <li>Lisp: <a href="lisp.corpus">lisp.corpus</a></li>
</ul>

<p>You can assume corpus files are of the form <code class="highlighter-rouge">&lt;source-name&gt;.corpus</code>.</p>

<h3 id="background">Background</h3>

<p>Note: this section is here for those interested in the background, but you don’t need to know it to complete this assignment. If you just want to get it done, feel free to skip to the Specific Requirements section.</p>

<p>In machine learning we train a model on some data and then use that model to make predictions about unseen instance of the same kind of data. For example, we can train a machine learning model on a data set consisting of several labeled images, some of which depict a dog and some of which don’t. We can then use the trained model to predict whether some unseen image (an image not in the training set) has a dog. The better the model, the better the accuracy (percentage of correct predictions) on unseen data.</p>

<p>We can create a model of language and use that model to predict the likelihood that some unseen text was generated by that model, in other words, how likely the unseen text is an example of the language modeled by the model. The model could be of a particular author, or a language such as French or English. One simple kind of model is well-suited to this problem: Markov chains.</p>

<p>Markov chains are useful for time-series or other sequential data. A Markov chain is a finite-state model in which the current state is dependent only on a bounded history of previous states.  In a first-order Markov chain the current state is dependent on only one previous state.</p>

<p>One can construct a simple first-order Markov chain of a language as the transition probabilities between letters in the language’s alphabet. For example, given this training corpus of a language:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BIG A, little a, what begins with A?
Aunt Annie’s alligator.
A...a...A

BIG B, little b, what begins with B?
Barber, baby, bubbles and a bumblebee.
</code></pre></div></div>

<p>We would have the model:</p>

<p><img src="drseuss-model.svg" alt="Dr Seuss Language Model" width="95%" /></p>

<p>We’ve only shown the letter-to-letter transitions that occur in the training corpus. Notice:</p>

<ul>
  <li>we’ve normalized to lowercase,</li>
  <li>we only consider letters – whitespace and punctuation are ignored</li>
  <li>the transition probabilities from one letter to all other letters sum to 1.0 (approximately, due to rounding),</li>
  <li>in a complete model, there would be arrows leading from all letters to all other letters, for <script type="math/tex">26^{26}</script> edges in a graph of 26 nodes, one for each letter, with unseen transitions labeled with 0.</li>
</ul>

<p>This model indicates that whenever the letter a occurs in our training corpus, the next letter is a, b, e, g, h, i, l, n, r, s, t, u or w. The arrow from a to b is labeled .19 because b appears after a 3 out of 16 times, or approximately 19 percent of the time. A first-order Markov chain is a kind of bigram model. Here are all the bigrams in the training text that begin with a, that is, all the state transitions from a:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(a, l), (a, w), (a, t), (a, a), (a, u), (a, n), (a, l), (a, t),
(a, a), (a, a), (a, b), (a, t), (a, r), (a, b), (a, n), (a, b)
</code></pre></div></div>

<p>A Markov chain represents all the bigrams and their probabilities of occurrence in the training corpus.</p>

<h4 id="representation-as-a-matrix">Representation as a matrix.</h4>

<p>A Markov chain can be represented as a transition matrix in which the probability of state j after state i is found in element (i, j) of the matrix. In the example below we have labeled the rows and columns with letters for readability. The probability of seeing the letter n after the letter a in the training corpus is found by entering row a and scanning across to column n, where we find the probability .12.</p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>a</td>
      <td>b</td>
      <td>c</td>
      <td>d</td>
      <td>e</td>
      <td>f</td>
      <td>g</td>
      <td>h</td>
      <td>i</td>
      <td>j</td>
      <td>k</td>
      <td>l</td>
      <td>m</td>
      <td>n</td>
      <td>o</td>
      <td>p</td>
      <td>q</td>
      <td>r</td>
      <td>s</td>
      <td>t</td>
      <td>u</td>
      <td>v</td>
      <td>w</td>
      <td>x</td>
      <td>y</td>
      <td>z</td>
    </tr>
    <tr>
      <td>a</td>
      <td>0.19</td>
      <td>0.19</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.06</td>
      <td>0.01</td>
      <td>0.19</td>
      <td>0.06</td>
      <td>0.01</td>
      <td>0.06</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>b</td>
      <td>0.12</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.24</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.18</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.06</td>
      <td>0.01</td>
      <td>0.06</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>c</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>d</td>
      <td>1.00</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>e</td>
      <td>0.11</td>
      <td>0.22</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.11</td>
      <td>0.01</td>
      <td>0.22</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.11</td>
      <td>0.22</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>f</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>g</td>
      <td>0.40</td>
      <td>0.20</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.40</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>h</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>i</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.10</td>
      <td>0.01</td>
      <td>0.30</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.20</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.40</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>j</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>k</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>l</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.50</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.38</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>m</td>
      <td>0.01</td>
      <td>1.00</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>n</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.17</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.17</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.17</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.33</td>
      <td>0.17</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>o</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>1.00</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>p</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>q</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>r</td>
      <td>0.33</td>
      <td>0.67</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>s</td>
      <td>0.50</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.50</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>t</td>
      <td>0.10</td>
      <td>0.20</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.20</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.20</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.10</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.20</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>u</td>
      <td>0.01</td>
      <td>0.33</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.33</td>
      <td>0.33</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>v</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>w</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.50</td>
      <td>0.50</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>x</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>y</td>
      <td>0.01</td>
      <td>1.00</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>z</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.01</td>
    </tr>
  </tbody>
</table>

<h4 id="prediction-using-a-markov-chain">Prediction using a Markov chain.</h4>

<p>Given a Markov chain model of a source, we can compute the probability that the model would produce a given string of letters by applying the chain rule. Simply stated, we walk the transitions in the Markov chain and multiply the transition probabilities. For example, the probability that “Big C, Little C” would be produced by our model, we would get the following probabilities from the transition matrix:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p(b, i) = .12
p(i, g) = .30
p(g, c) = .01
p(c, l) = .01
p(l, i) = .38
p(i, t) = .40
p(t, t) = .20
p(t, l) = .20
p(l, e) = .50
p(e, c) = .01
</code></pre></div></div>

<p>Multiplying them gives us 1.0588235294117648e-10. Notice that, in order to avoid getting zero-probability predictions using our simplified technique, we store .01 in our transition matrix for any bigram we don’t see in the training corpus. Also note that for larger test strings we would underflow the computer’s floating point representation and end up with zero probability.  There are techniques for avoiding this problem discussed in the Additional Information listed below.</p>

<h4 id="additional-information">Additional Information</h4>

<p>We’ve greatly simplified the presentation here to focus on the programming. For more information consult the following references.</p>

<ul>
  <li>Natural language processing: https://web.stanford.edu/~jurafsky/slp3/</li>
  <li>Markov chains: Chapter 11 of http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html, direct link: https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf</li>
</ul>

<h2 id="specific-requirements">Specific Requirements</h2>

<p>Write a class called <code class="highlighter-rouge">SourceModel</code> with the following constructors and methods:</p>

<ul>
  <li>
    <p>A single constructor with two <code class="highlighter-rouge">String</code> parameters, where the first parameter is the name of the source model and the second is the file name of the corpus file for the model. The constructor should create a letter-letter transition matrix using this recommended algorithm sketch:</p>

    <ul>
      <li>
        <p>Initialize a 26x26 matrix for character counts</p>
      </li>
      <li>
        <p>Print “Training {name} model … “</p>
      </li>
      <li>
        <p>Read the corpus file one character at a time, converting all characters to lower case and ignoring any non-alphabetic character.</p>
      </li>
      <li>
        <p>For each character, increment the corresponding (row, col) in your counts matrix. The row is the for the previous character, the col is for the current character. (You could also think of this in terms of bigrams.)</p>
      </li>
      <li>
        <p>After you read the entire corpus file, you’ll have a matrix of counts.</p>
      </li>
      <li>
        <p>From the matrix of counts, create a matrix of probabilities – each row of the transition matrix is a probability distribution.</p>

        <ul>
          <li>A probabilities in a distribution must sum to 1. To turn counts into probabilities, divide each count by the sum of all the counts in a row.</li>
        </ul>
      </li>
      <li>
        <p>Print “done.” followed by a newline character.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>A <code class="highlighter-rouge">getName</code> method with no parameters which returns the name of the <code class="highlighter-rouge">SourceModel</code>.</p>
  </li>
  <li>
    <p>A <code class="highlighter-rouge">toString</code> method which returns a <code class="highlighter-rouge">String</code> representation of the model like the one shown below under Running Your Program in jshell.</p>
  </li>
  <li>
    <p>A <code class="highlighter-rouge">probability</code> method which takes a <code class="highlighter-rouge">String</code> and returns a <code class="highlighter-rouge">double</code> which indicates the probability that the test string was generated by the source model, using the transition probability matrix created in the constructor. Here’s a recommended algorithm:</p>

    <ul>
      <li>Initialize the probability to 1.0</li>
      <li>For each two-character sequences of characters in the test string <code class="highlighter-rouge">test</code>, <script type="math/tex">c_i</script> and <script type="math/tex">c_{i+1}</script> for <script type="math/tex">i = 0</script> to <script type="math/tex">test.length() - 1</script>, multiply the probability by the entry in the transition probability matrix for the <script type="math/tex">c_1</script> to <script type="math/tex">c_2</script> transition, which should be found in row <script type="math/tex">c_i</script> an column <script type="math/tex">c_{i+1}</script> in the matrix. (You could also think of the indices as <script type="math/tex">c_{i-1}, c_i</script> for <script type="math/tex">i = 1</script> to <script type="math/tex">test.length() - 1</script>.)</li>
    </ul>
  </li>
  <li>
    <p>A <code class="highlighter-rouge">main</code> method that makes <code class="highlighter-rouge">SourceModel</code> runnable from the command line. You program should take 1 or more corpus file names as command line arguments followed by a quoted string as the last argument. The program should create models for all the corpora and test the string with all the corpora. Here’s an algorithm sketch:</p>

    <ul>
      <li>
        <p>The first n-1 arguments to the program are corpus file names to use to train models.  Corpus files are of the form <source-name>.corpus</source-name></p>
      </li>
      <li>
        <p>The last argument to the program is a quoted string to test.</p>
      </li>
      <li>
        <p>Create a SourceModel object for each corpus</p>
      </li>
      <li>
        <p>Use the models to compute the probability that the test text was produced by the model</p>
      </li>
      <li>
        <p>Probabilities will be very small. Normalize the probabilities of all the model predictions to a probability distribution (so they sum to 1) (closed-world assumption – we only state probabilities relative to models we have).</p>
      </li>
      <li>
        <p>Print results of analysis</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="running-your-program">Running Your Program</h2>

<p>Sample runs from the command line:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>java SourceModel <span class="k">*</span>.corpus <span class="s2">"If you got a gun up in your waist please don't shoot up the place (why?)"</span>
Training english model ... <span class="k">done</span><span class="nb">.</span>
Training french model ... <span class="k">done</span><span class="nb">.</span>
Training hiphop model ... <span class="k">done</span><span class="nb">.</span>
Training lisp model ... <span class="k">done</span><span class="nb">.</span>
Training spanish model ... <span class="k">done</span><span class="nb">.</span>
Analyzing: If you got a gun up <span class="k">in </span>your waist please don<span class="s1">'t shoot up the place (why?)
Probability that test string is  english: 0.00
Probability that test string is   french: 0.00
Probability that test string is   hiphop: 1.00
Probability that test string is     lisp: 0.00
Probability that test string is  spanish: 0.00
Test string is most likely hiphop.

$ java SourceModel *.corpus "Ou va le monde?"
Training english model ... done.
Training french model ... done.
Training hiphop model ... done.
Training lisp model ... done.
Training spanish model ... done.
Analyzing: Ou va le monde?
Probability that test string is  english: 0.02
Probability that test string is   french: 0.85
Probability that test string is   hiphop: 0.01
Probability that test string is     lisp: 0.10
Probability that test string is  spanish: 0.01
Test string is most likely french.

$ java SourceModel *.corpus "My other car is a cdr."
Training english model ... done.
Training french model ... done.
Training hiphop model ... done.
Training lisp model ... done.
Training spanish model ... done.
Analyzing: My other car is a cdr.
Probability that test string is  english: 0.39
Probability that test string is   french: 0.00
Probability that test string is   hiphop: 0.61
Probability that test string is     lisp: 0.00
Probability that test string is  spanish: 0.00
Test string is most likely hiphop.

$ java SourceModel *.corpus "defun Let there be rock"
Training english model ... done.
Training french model ... done.
Training hiphop model ... done.
Training lisp model ... done.
Training spanish model ... done.
Analyzing: defun Let there be rock
Probability that test string is  english: 0.01
Probability that test string is   french: 0.00
Probability that test string is   hiphop: 0.42
Probability that test string is     lisp: 0.57
Probability that test string is  spanish: 0.00
Test string is most likely lisp.
</span></code></pre></div></div>

<p>Sample runs from jshell:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>jshell
|  Welcome to JShell <span class="nt">--</span> Version 10.0.2
|  For an introduction <span class="nb">type</span>: /help intro

jshell&gt; /open SourceModel.java

jshell&gt; var french <span class="o">=</span> new SourceModel<span class="o">(</span><span class="s2">"french"</span>, <span class="s2">"french.corpus"</span><span class="o">)</span>
Training french model ... <span class="k">done</span><span class="nb">.</span>
french <span class="o">==&gt;</span> Model: french
     a    b    c    d    e    f     ...  1.00 0.01 0.01 0.01 0.01


jshell&gt; System.out.println<span class="o">(</span>french<span class="o">)</span> // implicitly calls french.toString<span class="o">()</span>
Model: french
     a    b    c    d    e    f    g    h    i    j    k    l    m    n    o    p    q    r    s    t    u    v    w    x    y    z
a 0.01 0.03 0.03 0.02 0.01 0.01 0.03 0.01 0.26 0.01 0.01 0.07 0.07 0.13 0.01 0.06 0.01 0.09 0.06 0.04 0.06 0.05 0.01 0.01 0.01 0.01
b 0.07 0.01 0.01 0.03 0.14 0.01 0.01 0.01 0.07 0.01 0.01 0.21 0.01 0.01 0.14 0.01 0.01 0.24 0.01 0.03 0.07 0.01 0.01 0.01 0.01 0.01
c 0.04 0.02 0.02 0.01 0.26 0.01 0.01 0.19 0.06 0.01 0.01 0.08 0.02 0.01 0.15 0.01 0.01 0.11 0.01 0.01 0.06 0.01 0.01 0.01 0.01 0.01
d 0.14 0.01 0.01 0.01 0.39 0.01 0.01 0.01 0.13 0.01 0.01 0.03 0.01 0.01 0.11 0.01 0.01 0.07 0.03 0.01 0.07 0.01 0.01 0.01 0.01 0.01
e 0.04 0.01 0.04 0.05 0.07 0.01 0.01 0.01 0.01 0.04 0.00 0.07 0.05 0.13 0.01 0.04 0.01 0.07 0.15 0.14 0.06 0.00 0.00 0.01 0.01 0.00
f 0.15 0.01 0.01 0.01 0.23 0.01 0.01 0.01 0.08 0.01 0.01 0.08 0.01 0.01 0.23 0.01 0.01 0.15 0.08 0.01 0.01 0.01 0.01 0.01 0.01 0.01
g 0.01 0.01 0.01 0.01 0.27 0.01 0.01 0.01 0.09 0.01 0.01 0.18 0.05 0.09 0.05 0.01 0.01 0.23 0.01 0.01 0.05 0.01 0.01 0.01 0.01 0.01
h 0.43 0.01 0.01 0.07 0.14 0.01 0.01 0.01 0.07 0.01 0.01 0.07 0.01 0.01 0.21 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
i 0.03 0.02 0.04 0.04 0.16 0.01 0.04 0.01 0.01 0.01 0.01 0.11 0.06 0.09 0.03 0.02 0.01 0.03 0.15 0.14 0.01 0.01 0.01 0.01 0.01 0.01
j 0.24 0.01 0.01 0.01 0.53 0.01 0.01 0.01 0.03 0.01 0.01 0.01 0.01 0.01 0.06 0.01 0.01 0.01 0.01 0.01 0.15 0.01 0.01 0.01 0.01 0.01
k 0.50 0.01 0.01 0.01 0.50 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
l 0.20 0.01 0.01 0.01 0.46 0.01 0.01 0.01 0.07 0.01 0.01 0.11 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.06 0.01 0.01 0.01 0.01 0.01
m 0.22 0.16 0.01 0.01 0.26 0.01 0.01 0.01 0.10 0.01 0.01 0.01 0.06 0.01 0.12 0.04 0.01 0.01 0.01 0.01 0.03 0.01 0.01 0.01 0.01 0.01
n 0.06 0.01 0.03 0.13 0.16 0.04 0.01 0.01 0.05 0.03 0.01 0.02 0.01 0.04 0.03 0.01 0.04 0.01 0.08 0.22 0.02 0.01 0.01 0.01 0.01 0.01
o 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.09 0.01 0.01 0.03 0.06 0.24 0.01 0.02 0.01 0.18 0.04 0.01 0.28 0.01 0.02 0.01 0.01 0.01
p 0.25 0.01 0.01 0.02 0.11 0.01 0.01 0.02 0.02 0.01 0.01 0.13 0.01 0.01 0.20 0.05 0.01 0.13 0.05 0.01 0.04 0.01 0.01 0.01 0.01 0.01
q 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 1.00 0.01 0.01 0.01 0.01 0.01
r 0.20 0.01 0.03 0.02 0.30 0.01 0.01 0.01 0.08 0.01 0.01 0.06 0.01 0.01 0.05 0.01 0.01 0.03 0.05 0.12 0.02 0.01 0.01 0.01 0.01 0.01
s 0.07 0.02 0.05 0.04 0.15 0.01 0.01 0.01 0.10 0.03 0.01 0.06 0.01 0.01 0.09 0.06 0.03 0.01 0.05 0.09 0.10 0.03 0.01 0.01 0.01 0.01
t 0.13 0.01 0.01 0.04 0.19 0.01 0.01 0.01 0.05 0.04 0.01 0.08 0.03 0.01 0.13 0.01 0.02 0.08 0.01 0.03 0.12 0.01 0.01 0.01 0.01 0.01
u 0.04 0.01 0.02 0.01 0.10 0.01 0.01 0.01 0.07 0.01 0.01 0.05 0.02 0.20 0.01 0.02 0.01 0.24 0.12 0.05 0.02 0.01 0.01 0.01 0.01 0.01
v 0.26 0.01 0.01 0.01 0.37 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.26 0.01 0.01 0.11 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
w 0.01 0.01 0.01 0.67 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.33 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
x 0.01 0.01 0.14 0.01 0.14 0.01 0.01 0.01 0.29 0.01 0.01 0.01 0.01 0.14 0.01 0.14 0.01 0.01 0.01 0.01 0.14 0.01 0.01 0.01 0.01 0.01
y 0.50 0.01 0.25 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.25 0.01 0.01 0.01 0.01 0.01 0.01 0.01
z 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 1.00 0.01 0.01 0.01 0.01

jshell&gt; french.probability<span class="o">(</span><span class="s2">"Il y a tout ce que vous voulez aux Champs-Elysees"</span><span class="o">)</span>
<span class="nv">$8</span> <span class="o">==&gt;</span> 3.966845096265183E-43
</code></pre></div></div>

<h2 id="grading">Grading</h2>

<ul>
  <li>[40 points] All required methods are present, code compiles and runs without causing a run-time exception.</li>
  <li>[10 points] Main method produces correctly formatted output (not necessarily correct numbers).</li>
  <li>[10 points] Correct transition probability matrix.</li>
  <li>[10 points] Correct probability calculation for test string.</li>
  <li>[10 points] Correct <code class="highlighter-rouge">toString</code>.</li>
  <li>[10 points] Relative probabilities for test string are correct.</li>
  <li>[10 points] Relative probabilities for test string are normalized to a probability distribution.</li>
  <li>[10 points] Correctly identifies most likely source.</li>
</ul>

<h2 id="tips-considerations-and-food-for-thought">Tips, Considerations and Food for Thought</h2>

<ul>
  <li>Refer to Oracle’s tutorial on reading a file one character at a time: <a href="https://docs.oracle.com/javase/tutorial/essential/io/charstreams.html">https://docs.oracle.com/javase/tutorial/essential/io/charstreams.html</a>
    <ul>
      <li><code class="highlighter-rouge">FileReader</code>’s <code class="highlighter-rouge">read</code> method returns <code class="highlighter-rouge">int</code>.  You’ll probably want to cast these to <code class="highlighter-rouge">char</code>s. That’s fine. As the documentation says, the lower 16 bits are the Unicode code point for a character.</li>
    </ul>
  </li>
  <li>
    <p>If you use <code class="highlighter-rouge">String.split</code> to get corpus names from file names, remember that <code class="highlighter-rouge">.</code> is a special regex character. Use a character class to match a literal <code class="highlighter-rouge">.</code> character. For example <code class="highlighter-rouge">"foo.fighters".split("[.]")</code> is <code class="highlighter-rouge">["foo", "fighters"]</code>.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">char</code> is an integral type, so you can easily find a <code class="highlighter-rouge">char</code>’s offset from <code class="highlighter-rouge">'a'</code> with an expression like <code class="highlighter-rouge">ch - 'a'</code>, where <code class="highlighter-rouge">ch</code> is a <code class="highlighter-rouge">char</code> variable.</p>
  </li>
  <li>
    <p>The <code class="highlighter-rouge">Character</code> class has many static utility methods you will find useful, like <code class="highlighter-rouge">isAlphabetic</code>, <code class="highlighter-rouge">toLowerCase</code>.</p>
  </li>
  <li>Have fun! Gather some copora of your own and try them out.</li>
</ul>

<h2 id="checkstyle">Checkstyle</h2>

<p>For each of your homework assignments we will run checkstyle and deduct one point for every checkstyle error.</p>

<p>For this homework the <strong>checkstyle cap is 10</strong>. This limit will increase with each homework.</p>

<ul>
  <li>If you encounter trouble running checkstyle, check Piazza for a solution and/or ask a TA as soon as you can!</li>
  <li>You can run checkstyle on your code by using the jar file found on the course website that includes xml configuration file specifying our checks. To check the style of your code run <code class="highlighter-rouge">java -jar checkstyle-6.2.2.jar *.java</code>.</li>
  <li>To check your Javadocs run <code class="highlighter-rouge">java -jar checkstyle-6.2.2.jar -j *.java</code>.</li>
  <li>Note that the command for checking code and the command for checking Javadocs are different. You will have to run both commands to fully test for style errors.</li>
  <li>Javadoc errors are the same as checkstyle errors, as in each one is worth a single point and they are counted towards the checkstyle cap.</li>
  <li><strong>You will be responsible for running checkstyle on <em>ALL</em> of your code.</strong></li>
  <li>Depending on your editor, you might be able to change some settings to make it easier to write style-compliant code. See the <a href="http://cs1331.gatech.edu/customization-tips.html">customization tips</a> page for more information.</li>
</ul>

<h2 id="collaboration">Collaboration</h2>

<p>When completing homeworks for CS1331 you may talk with other students about:</p>

<ul>
  <li>What general strategies or algorithms you used to solve problems in the homeworks</li>
  <li>Parts of the homework specification you are unsure of and need more explanation</li>
  <li>Online resources that helped you find a solution</li>
  <li>Key course concepts and Java language features used in your solution</li>
  <li><strong>You may not discuss, show, or share by other means the specifics of your code, including screenshots, file sharing, or showing someone else the code on your computer, or use code shared by others.</strong></li>
</ul>

<h3 id="examples-of-approveddisapproved-collaboration">Examples of approved/disapproved collaboration:</h3>

<p><strong>OKAY:</strong> “Hey, I’m really confused on how we are supposed to implement this part of the homework. What strategies/resources did you use to solve it?”</p>

<p><strong>BY NO MEANS OKAY:</strong> “Hey… the homework is due in like 20 minutes… Can I see your code? I <em>promise</em> won’t copy it directly!”</p>

<p>In addition to the above rules, note that it is not allowed to upload your code to any sort of public repository. This could be considered an Honor Code violation, even if it is after the homework is due.</p>

<h2 id="submission">Submission</h2>

<ul>
  <li>
    <p>Submit your Java source file as an attachment to the <code class="highlighter-rouge">hw2</code> assignment on Canvas. You can submit as many times as you want, so feel free to submit as you make substantial progress on the homework. We only grade your <strong>last</strong> submission, meaning we will ignore any previous submissions. Please be aware that Canvas will append a number at the end of the file name if multiple submissions are made (e.g. <code class="highlighter-rouge">YourFile-1.java</code>). We will take this into consideration as we grade and remove the appended number of the last submission.</p>
  </li>
  <li>
    <p>As always, late submissions will not be accepted and non-compiling code will be given a score of 0. For this reason, we recommend submitting early and then confirming that you submitted ALL of the necessary files by re-downloading your file(s) and compiling/running them.</p>
  </li>
</ul>

      </div>

  <!-- Bootstrap core JavaScript
  ================================================== -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="../../../js/bootstrap.min.js"></script>
  </body>
  </html>
